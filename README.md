# Automatic License Plate Recognition (ALPR) â€” Classical Morphology + OCR

> A lightweight ALPR pipeline that detects plates, enhances them with **morphological image processing**, and reads text with OCR. Built to run in realâ€‘time on roadway footage, with optional blacklist (watchlist) matching and clear visual overlays!

---

## ğŸ¬ Demo

[https://github.com/user-attachments/assets/14b95fe0-854a-4f81-be28-ff56e1cd322f](https://github.com/user-attachments/assets/14b95fe0-854a-4f81-be28-ff56e1cd322f)

---

## ğŸ” What this project does â€” at a glance

1. **Detect** candidate license plates in each frame (oneâ€‘stage CNN detector).
2. **Crop** each plate region and run a **morphology pipeline** to boost text contrast.
3. **OCR** the processed crop to obtain the plate string and a **confidence**.
4. **Overlay** the label, confidence, and a **green/red** ROI box (red on watchlist hit).
5. **Explainability:** draw tiny thumbnails of each processing step next to the vehicle.

<div align="left">
  <img src="https://github.com/user-attachments/assets/1ae88454-1fbf-4893-9395-ef175fa93a1a" width="500" alt="Close-up: stacked morphology thumbnails with OCR and confidence"/>
  <br/>
  <sub><b>Fig A.</b> Perâ€‘frame explainability: thumbnails of the morphological steps.</sub>
</div>

<div align="left">
  <img src="https://github.com/user-attachments/assets/c45867d7-a45e-4050-a469-40c3cf4c436b" width="500" alt="Multiâ€‘vehicle scene with ALPR overlays"/>
  <br/>
  <sub><b>Fig B.</b> Multiâ€‘vehicle scene â€” each detected plate is read and labeled with confidence.</sub>
</div>

<div align="left">
  <img src="https://github.com/user-attachments/assets/bd661eca-97d1-4e26-b833-01708ee8e631" width="500" alt="Street view with flagged and nonâ€‘flagged plates"/>
  <br/>
  <sub><b>Fig C.</b> Watchlist match example: nonâ€‘matches (green) vs a flagged plate (red).</sub>
</div>

---

## ğŸ§  Why classical morphology before OCR?

Traffic footage often contains **glare**, **motion blur**, and **low contrast** between characters and the plate background. OCR is far more reliable when characters are clearly separated from background. The pipeline therefore performs a short sequence on each plate crop:

* **Grayscale** â†’ remove color variation.
* **Gaussian blur** â†’ reduce sensor noise/fine texture.
* **Otsu threshold** â†’ pick a global binary threshold that best splits foreground letters from background.
* **Dilation** â†’ connect broken strokes with a small rectangular structuring element (2Ã—2 or 3Ã—3).
* **Invert** â†’ ensure polarity matches what the downstream OCR expects.

### Mathematical view, with plainâ€‘English meaning

**Otsuâ€™s threshold (global).**  For a threshold \$\tau\$, let \$\omega\_0(\tau)\$ and \$\omega\_1(\tau)\$ be the fractions of pixels classified as background and foreground; \$\mu\_0(\tau)\$ and \$\mu\_1(\tau)\$ their mean intensities; and \$\mu\$ the overall mean. Otsu chooses

$$
\tau^* = \arg\max_\tau\; \sigma_b^2(\tau),\quad\text{where}\quad \sigma_b^2(\tau)=\omega_0(\tau)\,\omega_1(\tau)\,\big(\mu_0(\tau)-\mu_1(\tau)\big)^2.
$$

**What this says:** among all possible cuts, pick the one that makes the two groups of pixels (text vs. plate) as far apart in brightness as possible. Maximizing betweenâ€‘class variance is equivalent to maximizing separability, which increases OCR legibility.

**Binary dilation.** For a binary image \$X\$ and a structuring element \$S\$,

$$
X \oplus S = \{\, x\;\mid\; (\hat S)_x \cap X \neq \emptyset \,\}.
$$

**What this says:** slide the (reflected) shape \$\hat S\$ across the image; a pixel becomes white if \$\hat S\$ touches any white pixel of \$X\$. Practically, this **fattens** character strokes so the OCR sees continuous letters instead of broken ones.

**Binary erosion.**

$$
X \ominus S = \{\, x\;\mid\; S_x \subseteq X \,\}.
$$

**What this says:** a pixel stays white only if the entire structuring element fits inside the white region. Erosion **shrinks** shapes and removes small specks.

**Opening and closing.**

$$
X \circ S = (X \ominus S) \oplus S,\qquad X \bullet S = (X \oplus S) \ominus S.
$$

**What this says:** opening (erode then dilate) removes tiny bright noise while preserving object shape; closing (dilate then erode) fills tiny dark holes and connects narrow gaps. Either can be swapped in for the single dilation step depending on footage quality.

**Morphological gradient (edge emphasis).**

$$
\nabla_S X = (X \oplus S) - (X \ominus S).
$$

**What this says:** take the difference between a fattened and a shrunken version of the image; the result highlights **edges**. On plates, it accentuates character boundaries before OCR.

---

## ğŸ§© Endâ€‘toâ€‘end pipeline (concept)

**Detector â†’ ROI â†’ Morphology â†’ OCR â†’ Postâ€‘process.**

**1) Detection.** A compact oneâ€‘stage detector outputs plate boxes $\[x\_1, y\_1, x\_2, y\_2]\$ and a class score per frame (initialized once at startup).

**2) ROI crop.** Each box is sanityâ€‘checked (aspect ratio / minimum size) and cropped. Very small crops are skipped to avoid spurious OCR.

**3) Morphological enhancement.** `morphological_pipeline(roi)` returns a list of labeled snapshots: `(Gray, Iâ‚)`, `(Blur, Iâ‚‚)`, `(Otsu, Iâ‚ƒ)`, `(Dilate, Iâ‚„)`, `(Invert, Iâ‚…)`. These are also used to render the explainer thumbnails in Fig A.

**4) OCR.** The enhanced crop is read by the OCR head. The output is an uppercase alphanumeric string and a confidence \$c \in \[0,1]\$.

**5) Postâ€‘processing & overlays.** The label and confidence are drawn in a legible banner above the box; the box is **green** by default and turns **red** if the plate string appears in a watchlist set. A vertical strip of thumbnails (from step 3) is placed near the vehicle for quick debugging.

**Highâ€‘level pseudocode**

```text
for each video frame:
  boxes = detector(frame)
  for b in boxes:
    roi = crop(frame, b)
    steps = morphological_pipeline(roi)   # [Gray, Blur, Otsu, Dilate, Invert]
    text, conf = ocr(select_best(steps))  # evaluate OCR on one/two variants
    color = RED if text in WATCHLIST else GREEN
    draw_banner(frame, text, conf, near=b)
    draw_box(frame, b, color)
    draw_thumbnails(frame, steps, column_near=b)
show(frame)
```

> **Selecting the best variant.** Confidence is evaluated on one or more processed snapshots (often the postâ€‘dilation or inverted image); if low, fall back to the raw crop.

---

## ğŸ§° Practical guidance & tuning

* **Thresholding:** Otsu works well for daytime clips. Under harsh glare or night scenes, adaptive thresholding (mean/gaussian) can replace or complement Otsu.
* **Structuring element:** Start with 2Ã—2 or 3Ã—3. Overâ€‘dilation can merge neighboring characters; underâ€‘dilation leaves gaps.
* **Text banner legibility:** Measure text size via `cv2.getTextSize` and draw a white rectangle behind the text to ensure readability across backgrounds.
* **Throughput:** Keep thumbnail sizes small (â‰ˆ80Ã—60) to retain realâ€‘time FPS while still providing explainability.
* **Watchlist:** Normalize OCR output by uppercasing and stripping spaces/hyphens before lookup.

---

## âœ… What to expect (typical results)

* On clear, forwardâ€‘facing shots: high read rates with confidences \$>0.9\$; stable overlays (Fig B).
* On lowâ€‘contrast plates: the morphology stage typically raises OCR confidence by connecting broken strokes (Fig A).
* On a watchlist hit: the ROI turns red and the label remains; false alarms are rare when confidence is high (Fig C).

**Limitations.** Motion blur and severe glare degrade detection and OCR; plate formats vary by region; stacked characters or nonâ€‘Latin scripts require specialized OCR. Adding simple tracking (IoUâ€‘based) stabilises labels across frames.

---

## ğŸ—‚ï¸ Repository (brief)

* `main.py` â€” video loop, detector inference, overlays, thumbnails, watchlist logic.
* `morphological_pipeline.py` â€” the classical enhancement chain described above.

**Ethics & privacy.** ALPR must be used in accordance with local law and organizational policy. Consider blurring faces/vehicles outside the ROI in demo footage.

---

## License

MIT â€” see `LICENSE`.
